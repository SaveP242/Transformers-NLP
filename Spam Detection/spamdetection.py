# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LJ8w91u4cScscIwWFlFKbNE0NHDo3Rb
"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline
import os
os.environ["CUDA_DEVICE_ORDER"]='PCI_BUS_ID';
os.environ["CUDA_VISIBLE_DEVICES"]='0';

pip install transformers

import pandas as pd
df=messages = pd.read_csv('SMSSpamCollection', sep='\t',
                           names=["label", "message"])
df.head()

X = list(df['message'])

y = list(df['label'])

y = list(pd.get_dummies(y, drop_first=True)['spam'])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=0.25)

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(X_train, truncation=True, padding=True)
test_encodings = tokenizer(X_test, truncation=True, padding=True)
val_en = tokenizer(X_val, truncation=True, padding=True)


import tensorflow as tf

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    y_train
))

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    y_test
))

val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_en),
    y_val
))


from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir='./results',  # output directory
    num_train_epochs=2,  # total number of training epochs
    per_device_train_batch_size=8,  # batch size per device during training
    per_device_eval_batch_size=16,  # batch size for evaluation
    warmup_steps=500,  # number of warmup steps for learning rate scheduler
    weight_decay=0.01,  # strength of weight decay
    logging_dir='./logs',  # directory for storing logs
    logging_steps=10,
    eval_steps=10
)

with training_args.strategy.scope():
    model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

trainer = TFTrainer(
    model=model,  # the instantiated Hugging Face Transformers model to be trained
    args=training_args,  # training arguments, defined above
    train_dataset=train_dataset,  # training dataset
    eval_dataset=val_dataset  # evaluation dataset
)

trainer.train()

trainer.evaluate(test_dataset)

trainer.predict(test_dataset)

trainer.predict(test_dataset)[1].shape

output=trainer.predict(test_dataset)[1]

import sklearn.metrics as met

model_acc = met.accuracy_score(y_test, output)
model_prec = met.precision_score(y_test, output)
model_recall = met.recall_score(y_test, output)
model_f1 = met.f1_score(y_test, output)
model_auc= met.roc_auc_score(y_test, output)
model_rmse= met.mean_squared_error(y_test, output)
print('acc:',model_acc,' prec:', model_prec, ' recall:', model_recall, ' f1:', model_f1, ' auc:',model_auc)

outcome = {"Messages":X_test, "Real Labels":y_test, "Predicted Labels":output}
df = pd.DataFrame(data=outcome)
df